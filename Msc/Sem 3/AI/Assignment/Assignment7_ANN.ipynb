{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement:\n",
    "ANN with Back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm:\n",
    "Step 1: Randomly Initialize the weghts and biases for the connections between neurons in the network, \n",
    "these initial values will be adjusted during the training process.\n",
    "\n",
    "Step 2: Pass input through the network to calculate the predicted output.\n",
    "\n",
    "Step 3: Apply Activation functions at each neuron to introduce non-linearity\n",
    "\n",
    "Step 4: Compare the predicted output with the actual target values to compute the loss(error) of the \n",
    "network.\n",
    "\n",
    "Step 5: Calculate the gradient of the loss w.r.t the weights and biases using the chain rule of calculus\n",
    "\n",
    "Step 6: Update weights and biases in the opposite direction of the gradient to minimize the loss. This step \n",
    "is where the term “backpropagation” comes from.\n",
    "\n",
    "Step 7: Use an optimization algorithm to update the weights and biases efficiently\n",
    "\n",
    "Step 8: Adjust the learning rate to control the size of weight updates.\n",
    "\n",
    "Step 9: Repeat steps 2-7 for a specified number of iterations or until the loss converges to a satisfactory \n",
    "level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input data 'X' as a NumPy array.\n",
    "# Each row represents an input example, and each column represents a feature.\n",
    "# In this case, we have 4 examples with 2 features each (binary inputs).\n",
    "X = np.array(([0, 0], [0, 1], [1, 0], [1, 1]), dtype=float)\n",
    "\n",
    "# Define the corresponding target output 'y' as a NumPy array.\n",
    "# Each row represents the expected output corresponding to the input example.\n",
    "# In this case, we have 4 expected outputs (binary values).\n",
    "y = np.array(([0], [1], [1], [0]), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The X array represents the input features, which are binary values (0 or 1)\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y array represents the corresponding expected output for each input example\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class named NeuralNetwork\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self):\n",
    "        # Initialize the network's architecture parameters\n",
    "        self.input = 2   # Number of input neurons\n",
    "        self.output = 1  # Number of output neurons\n",
    "        self.hidden = 3  # Number of neurons in the hidden layer\n",
    "        \n",
    "        # Initialize the weights for the neural network\n",
    "        self.W1 = np.random.randn(self.input, self.hidden)  # Weight matrix from input to hidden layer (random initialization)\n",
    "        self.W2 = np.random.randn(self.hidden, self.output)  # Weight matrix from hidden to output layer (random initialization)\n",
    "        \n",
    "    def feedForward(self, X):\n",
    "        # Perform forward propagation through the network\n",
    "        self.z = np.dot(X, self.W1)  # Calculate the dot product of inputs (X) and weights (W1)\n",
    "        self.z2 = self.sigmoid(self.z)  # Apply the sigmoid activation function to the result (z)\n",
    "        self.z3 = np.dot(self.z2, self.W2)  # Calculate the dot product of hidden layer outputs (z2) and weights (W2)\n",
    "        output = self.sigmoid(self.z3)  # Apply the sigmoid activation function to the result (z3)\n",
    "        return output\n",
    "        \n",
    "    def sigmoid(self, s, deriv=False):\n",
    "        # Define the sigmoid activation function and its derivative\n",
    "        if deriv:\n",
    "            return s * (1 - s)  # Derivative of the sigmoid function\n",
    "        return 1 / (1 + np.exp(-s))  # Sigmoid activation function\n",
    "    \n",
    "    def backward(self, X, y, output):\n",
    "        # Perform backward propagation through the network to update weights\n",
    "        self.output_error = y - output  # Calculate the error in the output\n",
    "        self.output_delta = self.output_error * self.sigmoid(output, deriv=True)  # Calculate the delta for the output layer\n",
    "        \n",
    "        self.z2_error = self.output_delta.dot(self.W2.T)  # Calculate the error for the hidden layer (z2)\n",
    "        self.z2_delta = self.z2_error * self.sigmoid(self.z2, deriv=True)  # Calculate the delta for the hidden layer\n",
    "        \n",
    "        self.W1 += X.T.dot(self.z2_delta)  # Update the weights of the first layer (input -> hidden)\n",
    "        self.W2 += self.z2.T.dot(self.output_delta)  # Update the weights of the second layer (hidden -> output)\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        output = self.feedForward(X)  # Perform forward propagation to get the output\n",
    "        self.backward(X, y, output)  # Perform backward propagation to update the weights\n",
    "\n",
    "# Create an instance of the NeuralNetwork class\n",
    "# This instance can be used to create and train a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0 : 0.43695284496380216\n",
      "Loss 100 : 0.18318881483555077\n",
      "Loss 200 : 0.07784239118644393\n",
      "Loss 300 : 0.03414898274093979\n",
      "Loss 400 : 0.020544989462296208\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the NeuralNetwork class\n",
    "NN = NeuralNetwork()\n",
    "\n",
    "# Loop through the training process for a specified number of epochs (500 times in this case)\n",
    "for i in range(500):\n",
    "    # Print the loss every 100 epochs\n",
    "    if i % 100 == 0:\n",
    "        # Calculate the loss by finding the mean squared error between the predicted output and the actual output\n",
    "        loss = np.mean(np.square(y - NN.feedForward(X)))\n",
    "        print(\"Loss\", i, \":\", loss)\n",
    "        \n",
    "    # Train the neural network using the training data (X) and target outputs (y)\n",
    "    NN.train(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      "[[0. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]]\n",
      "\n",
      "\n",
      "Actual Output: \n",
      " [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n",
      "\n",
      "\n",
      "Loss: \n",
      "0.014501925368449122\n",
      "\n",
      "\n",
      "Predicted Output: [[0.16008955]\n",
      " [0.88672642]\n",
      " [0.88642069]\n",
      " [0.08153448]]\n"
     ]
    }
   ],
   "source": [
    "# Print the input data\n",
    "print(\"Input: \\n\" + str(X))\n",
    "\n",
    "# Print a newline for separation\n",
    "print(\"\\n\")\n",
    "\n",
    "# Print the actual target output\n",
    "print(\"Actual Output: \\n\", y)\n",
    "\n",
    "# Print a newline for separation\n",
    "print(\"\\n\")\n",
    "\n",
    "# Calculate and print the loss, which is the mean squared error between the actual target output and the predicted output\n",
    "loss = np.mean(np.square(y - NN.feedForward(X)))\n",
    "print(\"Loss: \\n\" + str(loss))\n",
    "\n",
    "# Print a newline for separation\n",
    "print(\"\\n\")\n",
    "\n",
    "# Calculate and print the predicted output from the neural network\n",
    "predicted_output = NN.feedForward(X)\n",
    "print(\"Predicted Output: \" + str(predicted_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remarks\n",
    "The backpropagation algorithm is a gradient estimation method that uses the Leibniz chain rule to train \n",
    "neural network models. The gradient estimate is used by the optimization algorithm to compute the \n",
    "network parameter updates. The time complexity of the backpropagation algorithm for training artificial \n",
    "neural networks is O(nm), where n is the number of training examples and m is the number of weights in \n",
    "the neural network"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
